{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "saturday.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0aDI8q2Qb7g8VI08pa8Ve",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yohannesb/AdvSR/blob/master/saturday.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8IXjEqQ9TXNh",
        "outputId": "9fd29561-6452-4bcf-ad4a-4bfe5a45fdfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting update\n",
            "  Downloading update-0.0.1-py2.py3-none-any.whl (2.9 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting style==1.1.0\n",
            "  Downloading style-1.1.0-py2.py3-none-any.whl (6.4 kB)\n",
            "Installing collected packages: style, update\n",
            "Successfully installed style-1.1.0 update-0.0.1\n",
            "Found existing installation: kapre 0.3.6\n",
            "Uninstalling kapre-0.3.6:\n",
            "  Successfully uninstalled kapre-0.3.6\n",
            "Found existing installation: keras 2.7.0\n",
            "Uninstalling keras-2.7.0:\n",
            "  Successfully uninstalled keras-2.7.0\n",
            "Found existing installation: tensorflow-probability 0.15.0\n",
            "Uninstalling tensorflow-probability-0.15.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.15.0\n",
            "Found existing installation: albumentations 0.1.12\n",
            "Uninstalling albumentations-0.1.12:\n",
            "  Successfully uninstalled albumentations-0.1.12\n",
            "Found existing installation: datascience 0.10.6\n",
            "Uninstalling datascience-0.10.6:\n",
            "  Successfully uninstalled datascience-0.10.6\n",
            "Found existing installation: h5py 3.1.0\n",
            "Uninstalling h5py-3.1.0:\n",
            "  Successfully uninstalled h5py-3.1.0\n",
            "\u001b[33mWARNING: Skipping keras-nightly as it is not installed.\u001b[0m\n",
            "Cloning into 'nmt-keras'...\n",
            "remote: Enumerating objects: 4807, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 4807 (delta 31), reused 30 (delta 15), pack-reused 4730\u001b[K\n",
            "Receiving objects: 100% (4807/4807), 5.77 MiB | 23.75 MiB/s, done.\n",
            "Resolving deltas: 100% (3246/3246), done.\n",
            "Obtaining file:///content/nmt-keras\n",
            "Collecting keras@ https://github.com/MarcBS/keras/archive/master.zip\n",
            "  Downloading https://github.com/MarcBS/keras/archive/master.zip\n",
            "\u001b[K     - 101.2 MB 98.7 MB/s\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (0.16.0)\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras_preprocessing in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.1.2)\n",
            "Collecting h5py\n",
            "  Downloading h5py-3.6.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 16.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (3.2.2)\n",
            "Collecting multimodal-keras-wrapper\n",
            "  Downloading multimodal_keras_wrapper-3.1.6-py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.19.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (0.18.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.15.0)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (3.4.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.1.5)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nmt-keras==0.6) (1.4.1)\n",
            "Collecting tensorflow<2\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras@ https://github.com/MarcBS/keras/archive/master.zip->nmt-keras==0.6) (3.13)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.12.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.1.0)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.37.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.13.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (1.42.0)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 38.0 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 23.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2->nmt-keras==0.6) (3.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2->nmt-keras==0.6) (3.10.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nmt-keras==0.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nmt-keras==0.6) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nmt-keras==0.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->nmt-keras==0.6) (1.3.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.0)\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.29.24)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from multimodal-keras-wrapper->nmt-keras==0.6) (0.11.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->nmt-keras==0.6) (2018.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->nmt-keras==0.6) (2019.12.20)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->nmt-keras==0.6) (0.8.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->nmt-keras==0.6) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses->nmt-keras==0.6) (4.62.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->nmt-keras==0.6) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nmt-keras==0.6) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nmt-keras==0.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nmt-keras==0.6) (1.2.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nmt-keras==0.6) (2.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->nmt-keras==0.6) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nmt-keras==0.6) (3.0.0)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->nmt-keras==0.6) (2.7.3)\n",
            "Building wheels for collected packages: keras, gast\n",
            "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras: filename=Keras-2.3.1.1-py3-none-any.whl size=487513 sha256=144f4b78a523176d77cb3576267c7bcbd07f3d47ff09da1b6ad74e7cd3511602\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1jurt64u/wheels/68/86/9b/290dd8e0919a4070424e29c34886fbcf85d437c53506723c08\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=eab5905c6b99c10cf9375ad93ca705fab6520102cb36aee1d09363d4d9c4bed7\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built keras gast\n",
            "Installing collected packages: numpy, h5py, portalocker, mock, keras-applications, colorama, tensorflow-estimator, tensorboard, subword-nmt, sacremoses, sacrebleu, keras, gast, tensorflow, multimodal-keras-wrapper, nmt-keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Running setup.py develop for nmt-keras\n",
            "Successfully installed colorama-0.4.4 gast-0.2.2 h5py-2.10.0 keras-2.3.1.1 keras-applications-1.0.8 mock-4.0.3 multimodal-keras-wrapper-3.1.6 nmt-keras-0.6 numpy-1.18.5 portalocker-2.3.2 sacrebleu-2.0.0 sacremoses-0.0.46 subword-nmt-0.3.8 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install update pip\n",
        "!pip uninstall -y kapre keras tensorflow-probability albumentations datascience h5py keras-nightly # Avoid crashes with pre-installed packages \n",
        "!git clone https://github.com/lvapeab/nmt-keras\n",
        "import os\n",
        "os.chdir('nmt-keras')\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_wrapper.dataset import Dataset, saveDataset\n",
        "from data_engine.prepare_data import keep_n_captions\n",
        "ds = Dataset('tutorial_dataset', 'tutorial', silence=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbvmRjAqWxOZ",
        "outputId": "7d784ac6-9ee6-4faf-813a-b81c52e41132"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UN9g-tEnYr6P",
        "outputId": "ab3f5a0a-4623-4ace-9a20-7ab9c99249d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD9j8jX4Yv7n",
        "outputId": "66c6eeaa-367c-4179-b60d-b728299af058"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mnmt-keras\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd nmt-keras/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rbqnQ9mYy9P",
        "outputId": "04cb9432-7e72-4664-f0bb-71e31ca38e4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt-keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkxEeRtJY3pu",
        "outputId": "c119bb5d-5291-443d-c424-ca6232870fdc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py     \u001b[0m\u001b[01;34mexamples\u001b[0m/    \u001b[01;34mmeta-optimizers\u001b[0m/     sample_ensemble.py  \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mdata_engine\u001b[0m/  __init__.py  \u001b[01;34mnmt_keras\u001b[0m/           score.py\n",
            "\u001b[01;34mdemo-web\u001b[0m/     LICENSE      \u001b[01;34mnmt_keras.egg-info\u001b[0m/  setup.py\n",
            "\u001b[01;34mdocs\u001b[0m/         main.py      README.md            \u001b[01;34mtests\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd examples/  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnRJy4r2Y7nO",
        "outputId": "bd86172c-d475-4545-c75d-b4ed6da5d163"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt-keras/examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3PdowbGY_DG",
        "outputId": "b3b4b1b0-e7e9-4d7e-dfe0-1d62f515b799"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mconfigs\u001b[0m/        \u001b[01;34mEuTrans\u001b[0m/                 README.md\n",
            "\u001b[01;34mdocumentation\u001b[0m/  modeling_tutorial.ipynb  tutorial.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBdRDtxxZCmI",
        "outputId": "761b7c30-91c8-4d7a-850a-73767b742258"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nmt-keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.setOutput('examples/EuTrans/training.en',\n",
        "             'train',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             tokenization='tokenize_none',\n",
        "             build_vocabulary=True,\n",
        "             pad_on_batch=True,\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=30000,\n",
        "             min_occ=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc6L_sjGW8tR",
        "outputId": "bd707530-8335-4d35-deb6-854d3cfb45c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:15:07] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:15:07] Creating vocabulary for data with data_id 'target_text'.\n",
            "[18/12/2021 07:15:07] \t Total: 513 unique words in 9900 sentences with a total of 98304 words.\n",
            "[18/12/2021 07:15:07] Creating dictionary of 30000 most common words, covering 100.0% of the text.\n",
            "[18/12/2021 07:15:07] Loaded \"train\" set outputs of data_type \"text\" with data_id \"target_text\" and length 9900.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.setOutput('examples/EuTrans/dev.en',\n",
        "             'val',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             pad_on_batch=True,\n",
        "             tokenization='tokenize_none',\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egtXvIa4ZTGO",
        "outputId": "becd5d9b-05e9-4c91-8651-bc971f04f520"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:15:27] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:15:27] Loaded \"val\" set outputs of data_type \"text\" with data_id \"target_text\" and length 100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.setInput('examples/EuTrans/training.es',\n",
        "            'train',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            build_vocabulary=True,\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            max_words=30000,\n",
        "            min_occ=0)\n",
        "ds.setInput('examples/EuTrans/dev.es',\n",
        "            'val',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            min_occ=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEwOJ-i3ZVx0",
        "outputId": "89ec7185-a218-4f68-9193-4f274cf995a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:16:24] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:16:24] Creating vocabulary for data with data_id 'source_text'.\n",
            "[18/12/2021 07:16:24] \t Total: 686 unique words in 9900 sentences with a total of 96172 words.\n",
            "[18/12/2021 07:16:24] Creating dictionary of 30000 most common words, covering 100.0% of the text.\n",
            "[18/12/2021 07:16:24] Loaded \"train\" set inputs of data_type \"text\" with data_id \"source_text\" and length 9900.\n",
            "[18/12/2021 07:16:24] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:16:24] Loaded \"val\" set inputs of data_type \"text\" with data_id \"source_text\" and length 100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.setInput('examples/EuTrans/training.en',\n",
        "            'train',\n",
        "            type='text',\n",
        "            id='state_below',\n",
        "            required=False,\n",
        "            tokenization='tokenize_none',\n",
        "            pad_on_batch=True,\n",
        "            build_vocabulary='target_text',\n",
        "            offset=1,\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            max_words=30000)\n",
        "ds.setInput(None,\n",
        "            'val',\n",
        "            type='ghost',\n",
        "            id='state_below',\n",
        "            required=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rWGItqHZjOR",
        "outputId": "d9c2c3e3-372c-4a27-c7c4-e0c1d579ba68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:16:47] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:16:47] \tReusing vocabulary named \"target_text\" for data with data_id \"state_below\".\n",
            "[18/12/2021 07:16:47] Loaded \"train\" set inputs of data_type \"text\" with data_id \"state_below\" and length 9900.\n",
            "[18/12/2021 07:16:47] Loaded \"val\" set inputs of data_type \"ghost\" with data_id \"state_below\" and length 100.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for split, input_text_filename in zip(['train', 'val'], ['examples/EuTrans/training.es', 'examples/EuTrans/dev.es']):\n",
        "    ds.setRawInput(input_text_filename,\n",
        "                  split,\n",
        "                  type='file-name',\n",
        "                  id='raw_source_text',\n",
        "                  overwrite_split=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GuIlCEIZo0w",
        "outputId": "bc2686e1-d5a3-456b-8b23-970d47a77b12"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:17:08] Loaded \"train\" set inputs of type \"file-name\" with id \"raw_source_text\".\n",
            "[18/12/2021 07:17:08] Loaded \"val\" set inputs of type \"file-name\" with id \"raw_source_text\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_n_captions(ds, repeat=1, n=1, set_names=['val'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J7oihJIZtrp",
        "outputId": "df3672e1-a65c-4068-dc45-1a6efa1d58ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:17:30] Keeping 1 captions per input on the val set.\n",
            "[18/12/2021 07:17:30] Samples reduced to 100 in val set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saveDataset(ds, 'datasets')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzmZRNLnZ1AJ",
        "outputId": "60217baa-030c-40ae-e632-7ebf173e58df"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:17:52] <<< creating directory datasets ... >>>\n",
            "[18/12/2021 07:17:52] <<< Saving Dataset instance to datasets/Dataset_tutorial_dataset.pkl ... >>>\n",
            "[18/12/2021 07:17:52] <<< Dataset instance saved >>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from config import load_parameters\n",
        "from nmt_keras.model_zoo import TranslationModel\n",
        "from keras_wrapper.cnn_model import loadModel\n",
        "from keras_wrapper.dataset import loadDataset\n",
        "from keras_wrapper.extra.callbacks import PrintPerformanceMetricOnEpochEndOrEachNUpdates\n",
        "params = load_parameters()\n",
        "dataset = loadDataset('datasets/Dataset_tutorial_dataset.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHKwEILTZ4lt",
        "outputId": "363ebfaf-803d-4daf-e741-5f064c26bb0d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:18:24] <<< Cupy not available. Using numpy. >>>\n",
            "[18/12/2021 07:18:24] <<< Loading Dataset instance from datasets/Dataset_tutorial_dataset.pkl ... >>>\n",
            "[18/12/2021 07:18:24] <<< Dataset instance loaded >>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['source_text']\n",
        "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len['target_text']"
      ],
      "metadata": {
        "id": "1TEA7myuaACS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params['MODEL_TYPE'] = 'AttentionRNNEncoderDecoder' #  Supported models: 'AttentionRNNEncoderDecoder' and 'Transformer'.\n",
        "nmt_model = TranslationModel(params,\n",
        "                             model_type=params['MODEL_TYPE'], \n",
        "                             model_name='tutorial_model',\n",
        "                             vocabularies=dataset.vocabulary,\n",
        "                             store_path='trained_models/tutorial_model/',\n",
        "                             verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PptDbwv3aGgA",
        "outputId": "302e9305-72c1-4ada-b3ae-a860ba5fba94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:05] <<< Building AttentionRNNEncoderDecoder Translation_Model >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:650: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:05] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:650: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4786: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:05] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4786: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:157: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:05] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:157: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3561: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:06] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3561: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------\n",
            "\t\tTranslationModel instance\n",
            "-----------------------------------------------------------------------------------\n",
            "_model_type: AttentionRNNEncoderDecoder\n",
            "name: tutorial_model\n",
            "model_path: trained_models/tutorial_model/\n",
            "verbose: True\n",
            "\n",
            "Params:\n",
            "\tACCUMULATE_GRADIENTS: 1\n",
            "\tADDITIONAL_OUTPUT_MERGE_MODE: Add\n",
            "\tALIGN_FROM_RAW: True\n",
            "\tALPHA_FACTOR: 0.6\n",
            "\tAMSGRAD: False\n",
            "\tAPPLY_DETOKENIZATION: False\n",
            "\tATTENTION_DROPOUT_P: 0.0\n",
            "\tATTENTION_MODE: add\n",
            "\tATTENTION_SIZE: 32\n",
            "\tBATCH_NORMALIZATION_MODE: 1\n",
            "\tBATCH_SIZE: 50\n",
            "\tBEAM_SEARCH: True\n",
            "\tBEAM_SIZE: 6\n",
            "\tBETA_1: 0.9\n",
            "\tBETA_2: 0.999\n",
            "\tBIDIRECTIONAL_DEEP_ENCODER: True\n",
            "\tBIDIRECTIONAL_ENCODER: True\n",
            "\tBIDIRECTIONAL_MERGE_MODE: concat\n",
            "\tBPE_CODES_PATH: examples/EuTrans//training_codes.joint\n",
            "\tCLASSIFIER_ACTIVATION: softmax\n",
            "\tCLIP_C: 5.0\n",
            "\tCLIP_V: 0.0\n",
            "\tCOVERAGE_NORM_FACTOR: 0.2\n",
            "\tCOVERAGE_PENALTY: False\n",
            "\tDATASET_NAME: EuTrans\n",
            "\tDATASET_STORE_PATH: datasets/\n",
            "\tDATA_AUGMENTATION: False\n",
            "\tDATA_ROOT_PATH: examples/EuTrans/\n",
            "\tDECODER_HIDDEN_SIZE: 32\n",
            "\tDECODER_RNN_TYPE: ConditionalLSTM\n",
            "\tDEEP_OUTPUT_LAYERS: [('linear', 32)]\n",
            "\tDETOKENIZATION_METHOD: detokenize_none\n",
            "\tDOUBLE_STOCHASTIC_ATTENTION_REG: 0.0\n",
            "\tDROPOUT_P: 0.0\n",
            "\tEARLY_STOP: True\n",
            "\tEMBEDDINGS_FREQ: 1\n",
            "\tENCODER_HIDDEN_SIZE: 32\n",
            "\tENCODER_RNN_TYPE: LSTM\n",
            "\tEPOCHS_FOR_SAVE: 1\n",
            "\tEPSILON: 1e-08\n",
            "\tEVAL_EACH: 1\n",
            "\tEVAL_EACH_EPOCHS: True\n",
            "\tEVAL_ON_SETS: ['val']\n",
            "\tEXTRA_NAME: \n",
            "\tFF_SIZE: 128\n",
            "\tFILL: end\n",
            "\tFORCE_RELOAD_VOCABULARY: False\n",
            "\tGLOSSARY: None\n",
            "\tGRU_RESET_AFTER: True\n",
            "\tHEURISTIC: 0\n",
            "\tHOMOGENEOUS_BATCHES: False\n",
            "\tINIT_ATT: glorot_uniform\n",
            "\tINIT_FUNCTION: glorot_uniform\n",
            "\tINIT_LAYERS: ['tanh']\n",
            "\tINNER_INIT: orthogonal\n",
            "\tINPUTS_IDS_DATASET: ['source_text', 'state_below']\n",
            "\tINPUTS_IDS_MODEL: ['source_text', 'state_below']\n",
            "\tINPUTS_TYPES_DATASET: ['text-features', 'text-features']\n",
            "\tINPUT_VOCABULARY_SIZE: 689\n",
            "\tJOINT_BATCHES: 4\n",
            "\tKERAS_METRICS: ['perplexity']\n",
            "\tLABEL_SMOOTHING: 0.0\n",
            "\tLENGTH_NORM_FACTOR: 0.2\n",
            "\tLENGTH_PENALTY: False\n",
            "\tLOG_DIR: tensorboard_logs\n",
            "\tLOSS: categorical_crossentropy\n",
            "\tLR: 0.001\n",
            "\tLR_DECAY: None\n",
            "\tLR_GAMMA: 0.8\n",
            "\tLR_HALF_LIFE: 100\n",
            "\tLR_REDUCER_EXP_BASE: -0.5\n",
            "\tLR_REDUCER_TYPE: exponential\n",
            "\tLR_REDUCE_EACH_EPOCHS: False\n",
            "\tLR_START_REDUCTION_ON_EPOCH: 0\n",
            "\tMAPPING: examples/EuTrans//mapping.es_en.pkl\n",
            "\tMAXLEN_GIVEN_X: True\n",
            "\tMAXLEN_GIVEN_X_FACTOR: 2\n",
            "\tMAX_EPOCH: 500\n",
            "\tMAX_INPUT_TEXT_LEN: 50\n",
            "\tMAX_OUTPUT_TEXT_LEN: 50\n",
            "\tMAX_OUTPUT_TEXT_LEN_TEST: 150\n",
            "\tMAX_PLOT_Y: 100.0\n",
            "\tMETRICS: ['sacrebleu', 'perplexity']\n",
            "\tMINLEN_GIVEN_X: True\n",
            "\tMINLEN_GIVEN_X_FACTOR: 3\n",
            "\tMIN_DELTA: 0.0\n",
            "\tMIN_LR: 1e-09\n",
            "\tMIN_OCCURRENCES_INPUT_VOCAB: 0\n",
            "\tMIN_OCCURRENCES_OUTPUT_VOCAB: 0\n",
            "\tMODE: training\n",
            "\tMODEL_NAME: EuTrans_esen_AttentionRNNEncoderDecoder_src_emb_32_bidir_True_enc_LSTM_32_dec_ConditionalLSTM_32_deepout_linear_trg_emb_32_Adam_0.001\n",
            "\tMODEL_SIZE: 32\n",
            "\tMODEL_TYPE: AttentionRNNEncoderDecoder\n",
            "\tMOMENTUM: 0.0\n",
            "\tMULTIHEAD_ATTENTION_ACTIVATION: linear\n",
            "\tNESTEROV_MOMENTUM: False\n",
            "\tNOISE_AMOUNT: 0.01\n",
            "\tNORMALIZE_SAMPLING: False\n",
            "\tN_GPUS: 1\n",
            "\tN_HEADS: 8\n",
            "\tN_LAYERS_DECODER: 1\n",
            "\tN_LAYERS_ENCODER: 1\n",
            "\tN_SAMPLES: 5\n",
            "\tOPTIMIZED_SEARCH: True\n",
            "\tOPTIMIZER: Adam\n",
            "\tOUTPUTS_IDS_DATASET: ['target_text']\n",
            "\tOUTPUTS_IDS_MODEL: ['target_text']\n",
            "\tOUTPUTS_TYPES_DATASET: ['text-features']\n",
            "\tOUTPUT_VOCABULARY_SIZE: 516\n",
            "\tPAD_ON_BATCH: True\n",
            "\tPARALLEL_LOADERS: 1\n",
            "\tPATIENCE: 10\n",
            "\tPLOT_EVALUATION: False\n",
            "\tPOS_UNK: True\n",
            "\tREBUILD_DATASET: True\n",
            "\tRECURRENT_DROPOUT_P: 0.0\n",
            "\tRECURRENT_INPUT_DROPOUT_P: 0.0\n",
            "\tRECURRENT_WEIGHT_DECAY: 0.0\n",
            "\tREGULARIZATION_FN: L2\n",
            "\tRELOAD: 0\n",
            "\tRELOAD_EPOCH: True\n",
            "\tRHO: 0.9\n",
            "\tSAMPLE_EACH_UPDATES: 300\n",
            "\tSAMPLE_ON_SETS: ['train', 'val']\n",
            "\tSAMPLE_WEIGHTS: True\n",
            "\tSAMPLING: max_likelihood\n",
            "\tSAMPLING_SAVE_MODE: list\n",
            "\tSAVE_EACH_EVALUATION: True\n",
            "\tSCALE_SOURCE_WORD_EMBEDDINGS: False\n",
            "\tSCALE_TARGET_WORD_EMBEDDINGS: False\n",
            "\tSEARCH_PRUNING: False\n",
            "\tSKIP_VECTORS_HIDDEN_SIZE: 32\n",
            "\tSKIP_VECTORS_SHARED_ACTIVATION: tanh\n",
            "\tSOURCE_TEXT_EMBEDDING_SIZE: 32\n",
            "\tSRC_LAN: es\n",
            "\tSRC_PRETRAINED_VECTORS: None\n",
            "\tSRC_PRETRAINED_VECTORS_TRAINABLE: True\n",
            "\tSTART_EVAL_ON_EPOCH: 1\n",
            "\tSTART_SAMPLING_ON_EPOCH: 1\n",
            "\tSTOP_METRIC: Bleu_4\n",
            "\tSTORE_PATH: trained_models/EuTrans_esen_AttentionRNNEncoderDecoder_src_emb_32_bidir_True_enc_LSTM_32_dec_ConditionalLSTM_32_deepout_linear_trg_emb_32_Adam_0.001/\n",
            "\tTARGET_TEXT_EMBEDDING_SIZE: 32\n",
            "\tTASK_NAME: EuTrans\n",
            "\tTEMPERATURE: 1\n",
            "\tTENSORBOARD: True\n",
            "\tTEXT_FILES: {'train': 'training.', 'val': 'dev.', 'test': 'test.'}\n",
            "\tTIE_EMBEDDINGS: False\n",
            "\tTOKENIZATION_METHOD: tokenize_none\n",
            "\tTOKENIZE_HYPOTHESES: True\n",
            "\tTOKENIZE_REFERENCES: True\n",
            "\tTRAINABLE_DECODER: True\n",
            "\tTRAINABLE_ENCODER: True\n",
            "\tTRAIN_ON_TRAINVAL: False\n",
            "\tTRG_LAN: en\n",
            "\tTRG_PRETRAINED_VECTORS: None\n",
            "\tTRG_PRETRAINED_VECTORS_TRAINABLE: True\n",
            "\tUSE_BATCH_NORMALIZATION: True\n",
            "\tUSE_CUDNN: False\n",
            "\tUSE_L1: False\n",
            "\tUSE_L2: False\n",
            "\tUSE_NOISE: False\n",
            "\tUSE_PRELU: False\n",
            "\tUSE_TF_OPTIMIZER: True\n",
            "\tVERBOSE: 1\n",
            "\tWARMUP_EXP: -1.5\n",
            "\tWEIGHT_DECAY: 0.0001\n",
            "\tWRITE_VALID_SAMPLES: True\n",
            "-----------------------------------------------------------------------------------\n",
            "Model: \"tutorial_model_training\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "source_text (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "source_word_embedding (Embeddin (None, None, 32)     22048       source_text[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "src_embedding_batch_normalizati (None, None, 32)     128         source_word_embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "remove_mask_1 (RemoveMask)      (None, None, 32)     0           src_embedding_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_encoder_LSTM (Bid (None, None, 64)     16640       remove_mask_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "annotations_batch_normalization (None, None, 64)     256         bidirectional_encoder_LSTM[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "source_text_mask (GetMask)      (None, None, 32)     0           src_embedding_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "annotations (ApplyMask)         (None, None, 64)     0           annotations_batch_normalization[0\n",
            "                                                                 source_text_mask[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "state_below (InputLayer)        (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ctx_mean (MaskedMean)           (None, 64)           0           annotations[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "target_word_embedding (Embeddin (None, None, 32)     16512       state_below[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "initial_state (Dense)           (None, 32)           2080        ctx_mean[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "initial_memory (Dense)          (None, 32)           2080        ctx_mean[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "state_below_batch_normalization (None, None, 32)     128         target_word_embedding[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "initial_state_batch_normalizati (None, 32)           128         initial_state[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "initial_memory_batch_normalizat (None, 32)           128         initial_memory[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_AttConditionalLSTMCond  [(None, None, 32), ( 23873       state_below_batch_normalization[0\n",
            "                                                                 annotations[0][0]                \n",
            "                                                                 initial_state_batch_normalization\n",
            "                                                                 initial_memory_batch_normalizatio\n",
            "__________________________________________________________________________________________________\n",
            "proj_h0_batch_normalization (Ba (None, None, 32)     128         decoder_AttConditionalLSTMCond[0]\n",
            "__________________________________________________________________________________________________\n",
            "logit_ctx (TimeDistributed)     (None, None, 32)     2080        decoder_AttConditionalLSTMCond[0]\n",
            "__________________________________________________________________________________________________\n",
            "logit_lstm (TimeDistributed)    (None, None, 32)     1056        proj_h0_batch_normalization[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "permute_general_1 (PermuteGener (None, None, 32)     0           logit_ctx[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "logit_emb (TimeDistributed)     (None, None, 32)     1056        state_below_batch_normalization[0\n",
            "__________________________________________________________________________________________________\n",
            "out_layer_mlp_batch_normalizati (None, None, 32)     128         logit_lstm[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_ctx_batch_normalizati (None, None, 32)     128         permute_general_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_emb_batch_normalizati (None, None, 32)     128         logit_emb[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "additional_input (Add)          (None, None, 32)     0           out_layer_mlp_batch_normalization\n",
            "                                                                 out_layer_ctx_batch_normalization\n",
            "                                                                 out_layer_emb_batch_normalization\n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, None, 32)     0           additional_input[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "linear_0 (TimeDistributed)      (None, None, 32)     1056        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "out_layer_linear_0_batch_normal (None, None, 32)     128         linear_0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "target_text (TimeDistributed)   (None, None, 516)    17028       out_layer_linear_0_batch_normaliz\n",
            "==================================================================================================\n",
            "Total params: 106,917\n",
            "Trainable params: 106,213\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /content/nmt-keras/nmt_keras/model_zoo.py:213: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:06] From /content/nmt-keras/nmt_keras/model_zoo.py:213: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "[18/12/2021 07:19:07] Preparing optimizer and compiling. Optimizer configuration: \n",
            "\t LR: 0.001\n",
            "\t LOSS: categorical_crossentropy\n",
            "\t BETA_1: 0.9\n",
            "\t BETA_2: 0.999\n",
            "\t EPSILON: 1e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1192: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:19:07] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1192: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputMapping = dict()\n",
        "for i, id_in in enumerate(params['INPUTS_IDS_DATASET']):\n",
        "    pos_source = dataset.ids_inputs.index(id_in)\n",
        "    id_dest = nmt_model.ids_inputs[i]\n",
        "    inputMapping[id_dest] = pos_source\n",
        "nmt_model.setInputsMapping(inputMapping)\n",
        "\n",
        "outputMapping = dict()\n",
        "for i, id_out in enumerate(params['OUTPUTS_IDS_DATASET']):\n",
        "    pos_target = dataset.ids_outputs.index(id_out)\n",
        "    id_dest = nmt_model.ids_outputs[i]\n",
        "    outputMapping[id_dest] = pos_target\n",
        "nmt_model.setOutputsMapping(outputMapping)\n"
      ],
      "metadata": {
        "id": "IM6DFdhhaMG4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_transformer = params.get('ATTEND_ON_OUTPUT', 'transformer' in params['MODEL_TYPE'].lower())\n",
        "search_params = {\n",
        "    'language': 'en',\n",
        "    'tokenize_f': eval('dataset.' + 'tokenize_none'),\n",
        "    'beam_size': 12,\n",
        "    'optimized_search': True,\n",
        "    'model_inputs': params['INPUTS_IDS_MODEL'],\n",
        "    'model_outputs': params['OUTPUTS_IDS_MODEL'],\n",
        "    'dataset_inputs':  params['INPUTS_IDS_DATASET'],\n",
        "    'dataset_outputs':  params['OUTPUTS_IDS_DATASET'],\n",
        "    'n_parallel_loaders': 1,\n",
        "    'maxlen': 50,\n",
        "    'normalize_probs': True,\n",
        "    'pos_unk': True and not is_transformer,  # Pos_unk is unimplemented for transformer models\n",
        "    'heuristic': 0,\n",
        "    'state_below_maxlen': -1,\n",
        "    'attend_on_output': is_transformer,\n",
        "    'val': {'references': dataset.extra_variables['val']['target_text']}\n",
        "  }\n",
        "\n",
        "vocab = dataset.vocabulary['target_text']['idx2words']\n",
        "callbacks = []\n",
        "input_text_id = params['INPUTS_IDS_DATASET'][0]\n",
        "\n",
        "callbacks.append(PrintPerformanceMetricOnEpochEndOrEachNUpdates(nmt_model,\n",
        "                                                                dataset,\n",
        "                                                                gt_id='target_text',\n",
        "                                                                metric_name=['sacrebleu'],\n",
        "                                                                set_name=['val'],\n",
        "                                                                batch_size=50,\n",
        "                                                                each_n_epochs=1,\n",
        "                                                                extra_vars=search_params,\n",
        "                                                                reload_epoch=0,\n",
        "                                                                is_text=True,\n",
        "                                                                input_text_id=input_text_id,\n",
        "                                                                index2word_y=vocab,\n",
        "                                                                sampling_type='max_likelihood',\n",
        "                                                                beam_search=True,\n",
        "                                                                save_path=nmt_model.model_path,\n",
        "                                                                start_eval_on_epoch=0,\n",
        "                                                                write_samples=True,\n",
        "                                                                write_type='list',\n",
        "                                                                verbose=True))"
      ],
      "metadata": {
        "id": "4ZARsvqaaVoh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = {'n_epochs': 4,\n",
        "                   'batch_size': 50,\n",
        "                   'maxlen': 30,\n",
        "                   'epochs_for_save': 1,\n",
        "                   'verbose': 1,\n",
        "                   'eval_on_sets': [], \n",
        "                   'n_parallel_loaders': 1,\n",
        "                   'extra_callbacks': callbacks,\n",
        "                   'reload_epoch': 0,\n",
        "                   'epoch_offset': 0}"
      ],
      "metadata": {
        "id": "jMB4Amq7ac7S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nmt_model.trainNet(dataset, training_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZrhA1Nxahgb",
        "outputId": "da064ab5-bbe1-403b-ca15-b5e3dfb33f2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:55] <<< Training model >>>\n",
            "[18/12/2021 07:20:55] Training parameters: { \n",
            "\tbatch_size: 50\n",
            "\tclass_weights: None\n",
            "\tda_enhance_list: []\n",
            "\tda_patch_type: resize_and_rndcrop\n",
            "\tdata_augmentation: False\n",
            "\teach_n_epochs: 1\n",
            "\tepoch_offset: 0\n",
            "\tepochs_for_save: 1\n",
            "\teval_on_epochs: True\n",
            "\teval_on_sets: []\n",
            "\textra_callbacks: [<keras_wrapper.extra.callbacks.EvalPerformance object at 0x7f9181f30ed0>]\n",
            "\thomogeneous_batches: False\n",
            "\tinitial_lr: 1.0\n",
            "\tjoint_batches: 4\n",
            "\tlr_decay: None\n",
            "\tlr_gamma: 0.1\n",
            "\tlr_half_life: 50000\n",
            "\tlr_reducer_exp_base: 0.5\n",
            "\tlr_reducer_type: linear\n",
            "\tlr_warmup_exp: -1.5\n",
            "\tmaxlen: 30\n",
            "\tmean_substraction: False\n",
            "\tmetric_check: None\n",
            "\tmin_delta: 0.0\n",
            "\tmin_lr: 1e-09\n",
            "\tn_epochs: 4\n",
            "\tn_gpus: 1\n",
            "\tn_parallel_loaders: 1\n",
            "\tnormalization_type: None\n",
            "\tnormalize: False\n",
            "\tnum_iterations_val: None\n",
            "\tpatience: 0\n",
            "\tpatience_check_split: val\n",
            "\treduce_each_epochs: True\n",
            "\treload_epoch: 0\n",
            "\tshuffle: True\n",
            "\tstart_eval_on_epoch: 0\n",
            "\tstart_reduction_on_epoch: 0\n",
            "\ttensorboard: False\n",
            "\ttensorboard_params: {'log_dir': 'tensorboard_logs', 'histogram_freq': 0, 'batch_size': 50, 'write_graph': True, 'write_grads': False, 'write_images': False, 'embeddings_freq': 0, 'embeddings_layer_names': None, 'embeddings_metadata': None, 'update_freq': 'epoch'}\n",
            "\tverbose: 1\n",
            "\two_da_patch_type: whole\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3315: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:59] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3315: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:292: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:59] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:292: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:299: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:59] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:299: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:312: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:59] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:312: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:321: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:20:59] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:321: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:328: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:00] From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:328: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "198/198 [==============================] - 25s 127ms/step - loss: 1.8662 - perplexity: 121.2865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:26] <<< Saving model to trained_models/tutorial_model/epoch_1 ... >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
            "  'TensorFlow optimizers do not '\n",
            "[18/12/2021 07:21:29] <<< Model saved >>>\n",
            "\n",
            "[18/12/2021 07:21:29] <<< Predicting outputs of val set >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Total cost: 489.436122 \t Average cost: 4.894361\n",
            "The sampling took: 5.655296 secs (Speed: 0.056553 sec/sample)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:34] Prediction output 0: target_text (text)\n",
            "[18/12/2021 07:21:34] Decoding beam search prediction ...\n",
            "[18/12/2021 07:21:34] Using heuristic 0\n",
            "[18/12/2021 07:21:34] Evaluating on metric sacrebleu\n",
            "[18/12/2021 07:21:34] Computing SacreBleu scores on the val split...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:34] Bleu_4: 0.05642784418555766\n",
            "[18/12/2021 07:21:34] Done evaluating on metric sacrebleu\n",
            "[18/12/2021 07:21:34] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_1.jpg >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/4\n",
            "198/198 [==============================] - 22s 113ms/step - loss: 0.6436 - perplexity: 23.5672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:57] <<< Saving model to trained_models/tutorial_model/epoch_2 ... >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:21:57] <<< Model saved >>>\n",
            "\n",
            "[18/12/2021 07:21:57] <<< Predicting outputs of val set >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Total cost: 553.140292 \t Average cost: 5.531403\n",
            "The sampling took: 5.102958 secs (Speed: 0.051030 sec/sample)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:02] Prediction output 0: target_text (text)\n",
            "[18/12/2021 07:22:02] Decoding beam search prediction ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:02] Using heuristic 0\n",
            "[18/12/2021 07:22:02] Evaluating on metric sacrebleu\n",
            "[18/12/2021 07:22:02] Computing SacreBleu scores on the val split...\n",
            "[18/12/2021 07:22:02] Bleu_4: 36.83064873699548\n",
            "[18/12/2021 07:22:02] Done evaluating on metric sacrebleu\n",
            "[18/12/2021 07:22:02] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_2.jpg >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/4\n",
            "198/198 [==============================] - 22s 113ms/step - loss: 0.3914 - perplexity: 11.9120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:25] <<< Saving model to trained_models/tutorial_model/epoch_3 ... >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:25] <<< Model saved >>>\n",
            "\n",
            "[18/12/2021 07:22:25] <<< Predicting outputs of val set >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Total cost: 368.768164 \t Average cost: 3.687682\n",
            "The sampling took: 4.633711 secs (Speed: 0.046337 sec/sample)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:30] Prediction output 0: target_text (text)\n",
            "[18/12/2021 07:22:30] Decoding beam search prediction ...\n",
            "[18/12/2021 07:22:30] Using heuristic 0\n",
            "[18/12/2021 07:22:30] Evaluating on metric sacrebleu\n",
            "[18/12/2021 07:22:30] Computing SacreBleu scores on the val split...\n",
            "[18/12/2021 07:22:30] Bleu_4: 61.11924050237137\n",
            "[18/12/2021 07:22:30] Done evaluating on metric sacrebleu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:30] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_3.jpg >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/4\n",
            "198/198 [==============================] - 22s 113ms/step - loss: 0.2989 - perplexity: 7.9809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:52] <<< Saving model to trained_models/tutorial_model/epoch_4 ... >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:52] <<< Model saved >>>\n",
            "\n",
            "[18/12/2021 07:22:52] <<< Predicting outputs of val set >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Total cost: 239.296089 \t Average cost: 2.392961\n",
            "The sampling took: 4.370353 secs (Speed: 0.043704 sec/sample)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:57] Prediction output 0: target_text (text)\n",
            "[18/12/2021 07:22:57] Decoding beam search prediction ...\n",
            "[18/12/2021 07:22:57] Using heuristic 0\n",
            "[18/12/2021 07:22:57] Evaluating on metric sacrebleu\n",
            "[18/12/2021 07:22:57] Computing SacreBleu scores on the val split...\n",
            "[18/12/2021 07:22:57] Bleu_4: 73.87678870339836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:22:57] Done evaluating on metric sacrebleu\n",
            "[18/12/2021 07:22:57] \n",
            "<<< Progress plot saved in trained_models/tutorial_model/epoch_4.jpg >>>\n",
            "[18/12/2021 07:22:57] <<< Finished training model >>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.setInput('examples/EuTrans/test.es',\n",
        "            'test',\n",
        "            type='text',\n",
        "            id='source_text',\n",
        "            pad_on_batch=True,\n",
        "            tokenization='tokenize_none',\n",
        "            fill='end',\n",
        "            max_text_len=30,\n",
        "            min_occ=0)\n",
        "\n",
        "dataset.setInput(None,\n",
        "            'test',\n",
        "            type='ghost',\n",
        "            id='state_below',\n",
        "            required=False)\n",
        "\n",
        "dataset.setRawInput('examples/EuTrans/test.es',\n",
        "              'test',\n",
        "              type='file-name',\n",
        "              id='raw_source_text',\n",
        "              overwrite_split=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko0nHrMOcnu2",
        "outputId": "5d186dad-c4c6-4a59-828d-600d302a4f90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:35:39] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:35:39] Loaded \"test\" set inputs of data_type \"text\" with data_id \"source_text\" and length 2996.\n",
            "[18/12/2021 07:35:39] Loaded \"test\" set inputs of data_type \"ghost\" with data_id \"state_below\" and length 2996.\n",
            "[18/12/2021 07:35:39] Loaded \"test\" set inputs of type \"file-name\" with id \"raw_source_text\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['INPUTS_IDS_DATASET'][0]]\n",
        "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['OUTPUTS_IDS_DATASET'][0]]"
      ],
      "metadata": {
        "id": "qTr5KKObeA_M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "nmt_model = loadModel('trained_models/tutorial_model', 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg6KFpsZeCli",
        "outputId": "5068fe1d-5f16-4ccc-cfa7-a0ba7a24eb6b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:36:20] <<< Loading model from trained_models/tutorial_model/epoch_4_Model_Wrapper.pkl ... >>>\n",
            "[18/12/2021 07:36:20] <<< Loading model from trained_models/tutorial_model/epoch_4.h5 ... >>>\n",
            "[18/12/2021 07:36:22] <<< Loading optimized model... >>>\n",
            "[18/12/2021 07:36:28] <<< Optimized model loaded. >>>\n",
            "[18/12/2021 07:36:28] <<< Model loaded in 7.7805 seconds. >>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_transformer = params.get('ATTEND_ON_OUTPUT', 'transformer' in params['MODEL_TYPE'].lower())\n",
        "\n",
        "params_prediction = {\n",
        "    'language': 'en',\n",
        "    'tokenize_f': eval('dataset.' + 'tokenize_none'),\n",
        "    'beam_size': 12,\n",
        "    'optimized_search': True,\n",
        "    'model_inputs': params['INPUTS_IDS_MODEL'],\n",
        "    'model_outputs': params['OUTPUTS_IDS_MODEL'],\n",
        "    'dataset_inputs':  params['INPUTS_IDS_DATASET'],\n",
        "    'dataset_outputs':  params['OUTPUTS_IDS_DATASET'],\n",
        "    'n_parallel_loaders': 1,\n",
        "    'maxlen': 50,\n",
        "    'normalize_probs': True,\n",
        "    'pos_unk': True and not is_transformer,\n",
        "    'heuristic': 0,\n",
        "    'state_below_maxlen': -1,\n",
        "    'predict_on_sets': ['test'],\n",
        "    'verbose': 0,\n",
        "    'attend_on_output': is_transformer\n",
        "  }\n",
        "predictions = nmt_model.predictBeamSearchNet(dataset, params_prediction)['test']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiZib447eN1L",
        "outputId": "939b8f59-e35a-49e7-ffd2-74033d72e3fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "[18/12/2021 07:36:57] <<< Predicting outputs of test set >>>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Total cost: 9421.617930 \t Average cost: 3.144732\n",
            "The sampling took: 153.640150 secs (Speed: 0.051282 sec/sample)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_wrapper.utils import decode_predictions_beam_search\n",
        "vocab = dataset.vocabulary['target_text']['idx2words']\n",
        "samples = predictions['samples'] # Get word indices from the samples.\n",
        "\n",
        "predictions = decode_predictions_beam_search(samples,  \n",
        "                                             vocab,\n",
        "                                             verbose=params['VERBOSE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma9PhSSve97Y",
        "outputId": "8312d4ef-88c7-45ca-b876-9acd76f4dbf5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:40:19] Decoding beam search prediction ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = 'test.pred'\n",
        "from keras_wrapper.extra.read_write import list2file\n",
        "list2file(filepath, predictions)\n",
        "!head -n 4 test.pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhJfPP60fFTi",
        "outputId": "72cf6a25-6bac-4dba-a037-c41f3dc0b1e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I would like to book a room until tomorrow , please .\n",
            "please wake us up tomorrow at a quarter past seven .\n",
            "I am leaving today in the afternoon .\n",
            "would you mind sending down our luggage to room number three two six , please ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.setOutput('examples/EuTrans/test.en',\n",
        "             'test',\n",
        "             type='text',\n",
        "             id='target_text',\n",
        "             pad_on_batch=True,\n",
        "             tokenization='tokenize_none',\n",
        "             sample_weights=True,\n",
        "             max_text_len=30,\n",
        "             max_words=0)\n",
        "keep_n_captions(dataset, repeat=1, n=1, set_names=['test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doN2b1t3fS5z",
        "outputId": "46a1b354-c53a-42cd-f422-194953ac18e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:41:39] \tApplying tokenization function: \"tokenize_none\".\n",
            "[18/12/2021 07:41:39] Loaded \"test\" set outputs of data_type \"text\" with data_id \"target_text\" and length 2996.\n",
            "[18/12/2021 07:41:39] Keeping 1 captions per input on the test set.\n",
            "[18/12/2021 07:41:39] Samples reduced to 2996 in test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_wrapper.extra.evaluation import select\n",
        "metric = 'sacrebleu'\n",
        "# Apply sampling\n",
        "extra_vars = dict()\n",
        "extra_vars['tokenize_f'] = eval('dataset.' + 'tokenize_none')\n",
        "extra_vars['language'] = params['TRG_LAN']\n",
        "extra_vars['test'] = dict()\n",
        "extra_vars['test']['references'] = dataset.extra_variables['test']['target_text']\n",
        "metrics = select[metric](pred_list=predictions,\n",
        "                                          verbose=1,\n",
        "                                          extra_vars=extra_vars,\n",
        "                                          split='test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmTYxGs_fbcD",
        "outputId": "4338494d-6f02-4fdc-bf40-ffaff7b43640"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[18/12/2021 07:42:19] Computing SacreBleu scores on the test split...\n",
            "[18/12/2021 07:42:19] Bleu_4: 65.99690843438069\n"
          ]
        }
      ]
    }
  ]
}